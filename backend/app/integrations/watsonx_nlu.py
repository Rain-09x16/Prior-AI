"""
watsonx NLU Integration Wrapper

This module provides integration with IBM watsonx Natural Language Understanding (NLU)
for extracting structured information from invention disclosure documents.

IMPLEMENTATION STATUS: STUB/TODO
This is a stub implementation that needs to be completed by the AI/ML developer
with actual watsonx NLU API integration.

Required Environment Variables:
    WATSONX_API_KEY: Your IBM Cloud API key
    WATSONX_NLU_URL: watsonx NLU service endpoint
    WATSONX_PROJECT_ID: watsonx project ID

Usage:
    from app.integrations.watsonx_nlu import WatsonxNLU

    nlu = WatsonxNLU()
    result = nlu.analyze(text, features=['keywords', 'entities', 'concepts'])
"""

import os
from typing import Dict, List, Optional
from app.utils.logger import setup_logger

logger = setup_logger(__name__)

try:
    from ibm_watson import NaturalLanguageUnderstandingV1
    from ibm_watson.natural_language_understanding_v1 import Features, KeywordsOptions, EntitiesOptions, ConceptsOptions
    from ibm_cloud_sdk_core.authenticators import IAMAuthenticator
    NLU_AVAILABLE = True
except ImportError:
    NLU_AVAILABLE = False


class WatsonxNLU:
    """
    Wrapper for IBM watsonx Natural Language Understanding API.

    Used for:
    - Keyword extraction from disclosure documents
    - Entity recognition (technical terms, materials, measurements)
    - Concept extraction (high-level themes)
    - Sentiment and relevance scoring

    TODO: AI/ML Developer - Implement actual watsonx NLU integration
    """

    def __init__(self):
        """
        Initialize watsonx NLU client.

        Sets up actual watsonx NLU connection with credentials from environment variables.
        """
        self.api_key = os.getenv('WATSONX_API_KEY') or os.getenv('WATSONX_NLU_API_KEY')
        self.nlu_url = os.getenv('WATSONX_NLU_URL')
        self.nlu = None
        self.use_stub = False

        if not self.api_key:
            logger.warning("WATSONX_API_KEY or WATSONX_NLU_API_KEY not set - using stub implementation")
            self.use_stub = True
        elif not NLU_AVAILABLE:
            logger.warning("ibm-watson SDK not installed - using stub implementation. Install with: pip install ibm-watson")
            self.use_stub = True
        else:
            try:
                # Initialize watsonx NLU authenticator
                authenticator = IAMAuthenticator(apikey=self.api_key)
                
                self.nlu = NaturalLanguageUnderstandingV1(
                    version='2022-04-07',
                    authenticator=authenticator
                )
                
                # Set service URL if provided (for on-premise deployments)
                if self.nlu_url:
                    self.nlu.set_service_url(self.nlu_url)
                else:
                    # Default to US South region
                    self.nlu.set_service_url('https://api.us-south.natural-language-understanding.watson.cloud.ibm.com')
                
                logger.info("WatsonxNLU initialized - REAL API mode")
            except Exception as e:
                logger.error(f"Failed to initialize watsonx NLU client: {e}")
                logger.warning("Falling back to stub implementation")
                self.use_stub = True

    def analyze(self, text: str, features: Optional[List[str]] = None) -> Dict:
        """
        Analyze text using watsonx NLU.

        Args:
            text: Text to analyze (disclosure document)
            features: List of features to extract. Options:
                - 'keywords': Important keywords with relevance scores
                - 'entities': Named entities (people, organizations, technical terms)
                - 'concepts': High-level concepts and themes
                - 'categories': Document categories
                - 'sentiment': Overall sentiment

        Returns:
            Dictionary with extracted features:
            {
                'keywords': [{'text': 'keyword', 'relevance': 0.95}, ...],
                'entities': [{'type': 'Technology', 'text': 'lithium-ion battery', 'relevance': 0.89}, ...],
                'concepts': [{'text': 'energy storage', 'relevance': 0.85}, ...],
                'categories': [{'label': '/technology and computing', 'score': 0.92}],
                'sentiment': {'score': 0.5, 'label': 'neutral'}
            }

        """
        if features is None:
            features = ['keywords', 'entities', 'concepts']

        if self.use_stub or not self.nlu:
            logger.info(f"Analyzing text with features: {features} (STUB - no API available)")
            return self._stub_analyze(text, features)

        try:
            logger.info(f"Analyzing text with watsonx NLU, features: {features}, text length: {len(text)} chars")
            
            # Build features for the API call
            feature_params = {}
            
            if 'keywords' in features:
                feature_params['keywords'] = KeywordsOptions(limit=20)
            if 'entities' in features:
                feature_params['entities'] = EntitiesOptions(limit=50)
            if 'concepts' in features:
                feature_params['concepts'] = ConceptsOptions(limit=10)
            
            # Call watsonx NLU API (limit text to 20KB to avoid token limit issues)
            response = self.nlu.analyze(
                text=text[:20000],
                features=Features(**feature_params)
            ).get_result()
            
            logger.info(f"Successfully analyzed text with watsonx NLU")
            return response
            
        except Exception as e:
            logger.error(f"Error analyzing text with watsonx NLU: {str(e)}")
            logger.warning("Falling back to stub implementation")
            return self._stub_analyze(text, features)



    def _stub_analyze(self, text: str, features: List[str]) -> Dict:
        """
        STUB implementation - returns mock NLU data.

        This allows the system to work in development mode without actual
        watsonx credentials. Remove when implementing real integration.
        """
        import re

        result = {}

        # Extract keywords (simple word frequency)
        if 'keywords' in features:
            words = re.findall(r'\b[a-z]{4,}\b', text.lower())
            common_words = {'this', 'that', 'with', 'from', 'have', 'been', 'were', 'will', 'into', 'than', 'they', 'their', 'which', 'would', 'about'}
            word_freq = {}
            for word in words:
                if word not in common_words:
                    word_freq[word] = word_freq.get(word, 0) + 1

            # Get top keywords
            top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:20]
            result['keywords'] = [
                {'text': word, 'relevance': min(0.99, 0.5 + (count / 20))}
                for word, count in top_words
            ]

        # Extract entities (simple pattern matching)
        if 'entities' in features:
            result['entities'] = []

            # Look for measurements
            measurements = re.findall(r'\d+(?:\.\d+)?(?:-\d+(?:\.\d+)?)?\s*(?:[Â°%]?[A-Za-z]+)', text[:2000])
            for i, measurement in enumerate(measurements[:10]):
                result['entities'].append({
                    'type': 'Measurement',
                    'text': measurement,
                    'relevance': 0.8 - (i * 0.05)
                })

        # Extract concepts (simple heuristic)
        if 'concepts' in features:
            result['concepts'] = [
                {'text': 'innovation', 'relevance': 0.85},
                {'text': 'technology', 'relevance': 0.75},
                {'text': 'technical advancement', 'relevance': 0.65}
            ]

        return result

    def extract_keywords(self, text: str, limit: int = 20) -> List[str]:
        """
        Extract keywords from text.

        Convenience method that calls analyze() and returns just the keywords.

        Args:
            text: Text to analyze
            limit: Maximum number of keywords to return

        Returns:
            List of keyword strings
        """
        result = self.analyze(text, features=['keywords'])
        keywords = result.get('keywords', [])
        return [kw['text'] for kw in keywords[:limit]]


# TODO: AI/ML Developer - Complete Implementation Checklist
#
# [ ] Install required dependencies:
#     pip install ibm-watson ibm-cloud-sdk-core
#
# [ ] Set up watsonx NLU credentials in .env:
#     WATSONX_API_KEY=your_actual_key
#     WATSONX_NLU_URL=https://api.us-south.natural-language-understanding.watson.cloud.ibm.com
#     WATSONX_PROJECT_ID=your_project_id
#
# [ ] Replace __init__() with actual NLU client initialization
#
# [ ] Replace analyze() stub with actual watsonx NLU API calls
#
# [ ] Test with sample disclosure documents
#
# [ ] Handle API errors and rate limiting
#
# [ ] Add retry logic for transient failures
#
# [ ] Consider caching NLU results to reduce API calls
